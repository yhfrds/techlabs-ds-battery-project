{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b16aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f165bb26",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "### Reading and Cleaning Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4a1cc4",
   "metadata": {},
   "source": [
    "### Imputing Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc89764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # This is necessary to enable IterativeImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "missing_before = df.isna().sum()\n",
    "\n",
    "# Initialize IterativeImputer (MICE method)\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "\n",
    "# Impute missing values and convert back to DataFrame with original columns\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), \n",
    "                                               columns=df.columns)\n",
    "\n",
    "# Clip negative values to 0 for all columns after imputation, except 'Dew_Point_Temperature' and 'Air_Temperature', \n",
    "# as these columns can validly contain negative values (e.g., for temperatures below freezing).\n",
    "columns_to_clip = df_imputed.columns.difference(['Dew_Point_Temperature', 'Air_Temperature'])\n",
    "\n",
    "# Apply clipping for all columns except the ones in the exclusion list\n",
    "df_imputed[columns_to_clip] = df_imputed[columns_to_clip].clip(lower=0)\n",
    "\n",
    "# Restore the original index and update the original dataframe\n",
    "df_imputed.index = df.index\n",
    "\n",
    "missing_after = df_imputed.isna().sum()\n",
    "\n",
    "# Output the before and after missing values for each column\n",
    "missing_values_comparison = pd.DataFrame({\n",
    "    'Before Imputation': missing_before,\n",
    "    'After Imputation': missing_after\n",
    "})\n",
    "\n",
    "print(missing_values_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51e5c43c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/afeli/Desktop/Residual load - datasets/DATASET_final_untransformed_uncleaned.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load dataset and convert 'Date' column to datetime format\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/afeli/Desktop/Residual load - datasets/DATASET_final_untransformed_uncleaned.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Keep only the data from June 1, 2018, onwards, as the global radiation dataset starts from this date (reported by Celine)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/afeli/Desktop/Residual load - datasets/DATASET_final_untransformed_uncleaned.csv'"
     ]
    }
   ],
   "source": [
    "# Load dataset and convert 'Date' column to datetime format\n",
    "df = pd.read_csv('../data/DATASET_final_untransformed_uncleaned.csv', sep=\";\")\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "# Keep only the data from June 1, 2018, onwards, as the global radiation dataset starts from this date (reported by Celine)\n",
    "df = df[df[\"Date\"] >= '2018-06-18']\n",
    "\n",
    "# Set 'Date' as the index for time-based operations\n",
    "df.set_index(\"Date\", inplace=True)\n",
    "\n",
    "# Define valid value ranges for data cleaning\n",
    "valid_ranges = {\n",
    "    'Air_Temperature': (-90, 60),\n",
    "    'Relative_Humidity': (0, 100),\n",
    "    'Visibility': (0, 10),\n",
    "    'Air_Pressure_at_Station_Height': (870, 1085),\n",
    "    'Cloud_Cover': (0, 8),\n",
    "    'Daily_Precipitation_Height': (0, np.inf),\n",
    "    'Snow_Height_Daily_Value': (0, np.inf),\n",
    "    'Global_Radiation': (0.1, np.inf),\n",
    "    'Vapor_Pressure': (0, 70),\n",
    "    'Dew_Point_Temperature': (-80, 30),\n",
    "    'Wind_Direction': (0, 31),\n",
    "    'Wind_Strength': (0, 12),\n",
    "}\n",
    "\n",
    "def clean_column(x, col_name):\n",
    "    \"\"\"Replace values outside the valid range with NaN.\"\"\"\n",
    "    min_val, max_val = valid_ranges.get(col_name, (None, None))\n",
    "    return x.where((x >= min_val) & (x <= max_val), np.nan) if min_val is not None else x\n",
    "\n",
    "# Apply cleaning function to all columns\n",
    "for col in df.columns:\n",
    "    df[col] = clean_column(df[col], col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59a1f2d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up the figure and axes\n",
    "num_columns = len(df_imputed.columns)\n",
    "fig, axes = plt.subplots(num_columns, 1, figsize=(num_columns, num_columns*1.5))  # Adjust figsize as needed\n",
    "\n",
    "# # Define a color palette (you can adjust the palette)\n",
    "palette = sns.color_palette(\"dark\", n_colors=num_columns)\n",
    "\n",
    "# # Loop through each column and plot it in the corresponding subplot with a different color\n",
    "for i, column in enumerate(df_imputed.columns):\n",
    "     sns.lineplot(data=df_imputed, x=df_imputed.index, y=column, ax=axes[i], color=palette[i])\n",
    "     axes[i].set_title(column)\n",
    "     axes[i].set_xlabel('Date')\n",
    "     axes[i].set_ylabel('Value')\n",
    "     axes[i].grid(True)\n",
    "\n",
    "# # Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# # Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fab722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set Date as the index for correlation\n",
    "scale = 8\n",
    "# # Create a correlation matrix\n",
    "corr_matrix = df_imputed.corr()\n",
    "plt.figure(figsize=(2*scale, 1*scale))\n",
    "# # Create a heatmap to visualize the correlation matrix\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm',\n",
    "             fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# # Show the plot\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dbfe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Define the periods you want to plot\n",
    "periods = [\n",
    "     (\"2018-12\", \"2019-01\"),\n",
    "     (\"2019-12\", \"2020-01\"),\n",
    "     (\"2020-12\", \"2021-01\"),\n",
    "     (\"2021-12\", \"2022-01\"),\n",
    "     (\"2022-12\", \"2023-01\"),\n",
    "     (\"2023-12\", \"2024-01\"),\n",
    "     (\"2024-12\", \"2025-01\")\n",
    " ]\n",
    "\n",
    "# # Create subplots (7 rows, 1 column, as you have 7 periods)\n",
    "fig, ax = plt.subplots(len(periods), 1, figsize=(15, 20))\n",
    "\n",
    "# # Loop through periods and plot each on a separate subplot\n",
    "for i, (start_date, end_date) in enumerate(periods):\n",
    "     df['Residual load [MWh] Calculated resolutions'][start_date:end_date].plot(\n",
    "         kind='line', ax=ax[i], title=f\"Residual Load ({start_date} to {end_date})\"\n",
    "     )\n",
    "     ax[i].set_xlabel(\"Dates\")\n",
    "     ax[i].set_ylabel(\"Residual Load (MWh)\")\n",
    "\n",
    "# # Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b51c00",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### Feature Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b432cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "corr_matrix = df_imputed.corr().abs()  # Use absolute values to check both positive & negative correlation\n",
    "\n",
    "# Select upper triangle of correlation matrix (to avoid duplicate pairs)\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find feature pairs with correlation above a threshold (e.g., 0.85)\n",
    "threshold = 0.85\n",
    "highly_correlated_pairs = []\n",
    "\n",
    "for col in upper.columns:\n",
    "    for row in upper.index:\n",
    "        if upper.loc[row, col] > threshold:\n",
    "            highly_correlated_pairs.append((row, col, upper.loc[row, col]))\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "correlated_df = pd.DataFrame(highly_correlated_pairs, columns=[\"Feature 1\", \"Feature 2\", \"Correlation\"])\n",
    "correlated_df = correlated_df.sort_values(by=\"Correlation\", ascending=False)\n",
    "\n",
    "# Display results\n",
    "print(\"Highly Correlated Feature Pairs (Threshold > 0.85):\")\n",
    "display(correlated_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff2b4e2",
   "metadata": {},
   "source": [
    "### Adding Features (Month, Year, Holiday, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a438ca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe to store the transformed data\n",
    "df_transformed = df_imputed.copy()\n",
    "\n",
    "# Add year, month, and day feature\n",
    "df_transformed['dayofweek'] = df_transformed.index.dayofweek\n",
    "df_transformed['quarter'] = df_transformed.index.quarter\n",
    "df_transformed['month'] = df_transformed.index.month\n",
    "df_transformed['year'] = df_transformed.index.year\n",
    "df_transformed['dayofyear'] = df_transformed.index.dayofyear\n",
    "\n",
    "def check_holiday(date):\n",
    "    if (date.month == 12 and date.day == 24):  # Weihnachten (Christmas Eve)\n",
    "        return 'Weihnachten'\n",
    "    elif (date.month == 12 and date.day == 31):  # Silvester (New Year's Eve)\n",
    "        return 'Silvester'\n",
    "    else:\n",
    "        return 'No_Holiday'\n",
    "\n",
    "df_transformed['holiday'] = df_transformed.index.to_series().apply(check_holiday)\n",
    "df_transformed['holiday'] = df_transformed['holiday'].astype(\"category\")\n",
    "\n",
    "# Removing Dew_Point_Temperature and Vapor_Pressure because of high correlation with air temperature and air temperature correlate more with the residual load\n",
    "df_transformed = df_transformed.drop([\"Dew_Point_Temperature\", \"Vapor_Pressure\"], axis=1)\n",
    "\n",
    "# Convert wind direction to radians for calculations\n",
    "df_transformed['Wind_Direction_rad'] = np.deg2rad(df_transformed['Wind_Direction'])\n",
    "\n",
    "# Calculate u and v components\n",
    "df_transformed['wind_u'] = df_transformed['Wind_Strength'] * np.sin(df_transformed['Wind_Direction_rad'])\n",
    "df_transformed['wind_v'] = df_transformed['Wind_Strength'] * np.cos(df_transformed['Wind_Direction_rad'])\n",
    "\n",
    "# Drop some columns that is not needed\n",
    "df_transformed = df_transformed.drop(columns=['Wind_Direction_rad', \"Wind_Strength\", 'Wind_Direction'], axis=1)\n",
    "\n",
    "# Rename some columns\n",
    "rename_dict = {\n",
    "    'Total (grid load) [MWh] Calculated resolutions': 'Total_Load',\n",
    "    'Residual load [MWh] Calculated resolutions': 'Residual_Load',\n",
    "    'Hydro pumped storage [MWh] Calculated resolutions': 'Hydro_Pumped_Storage'\n",
    "}\n",
    "df_transformed.rename(columns=rename_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7296d368",
   "metadata": {},
   "source": [
    "## Combine Datasets (Electricity Generation & Weather Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60e5041",
   "metadata": {},
   "outputs": [],
   "source": [
    "trsnet_generation = pd.read_csv(\n",
    "    \"C:/Users/afeli/Desktop/Residual load - datasets/TransnetBW_electricity_actual_generation_day_150101_250320.csv\",\n",
    "    usecols=['Start date', 'Wind onshore [MWh] Calculated resolutions', 'Photovoltaics [MWh] Calculated resolutions'], delimiter=\";\",thousands=\",\"  \n",
    ")\n",
    "trsnet_generation['Start date'] = pd.to_datetime(trsnet_generation['Start date'], format='%b %d, %Y')\n",
    "trsnet_generation = trsnet_generation[trsnet_generation['Start date'] <= \"2025-02-04\"]\n",
    "trsnet_generation.set_index('Start date', inplace=True)\n",
    "\n",
    "# Ensure numeric data type for summation\n",
    "trsnet_generation['Wind onshore [MWh] Calculated resolutions'] = trsnet_generation['Wind onshore [MWh] Calculated resolutions'].str.replace(',', '')\n",
    "trsnet_generation['Wind onshore [MWh] Calculated resolutions'] = pd.to_numeric(trsnet_generation['Wind onshore [MWh] Calculated resolutions'], errors='coerce')\n",
    "trsnet_generation['Photovoltaics [MWh] Calculated resolutions'] = trsnet_generation['Photovoltaics [MWh] Calculated resolutions'].str.replace(',', '')\n",
    "trsnet_generation['Photovoltaics [MWh] Calculated resolutions'] = pd.to_numeric(trsnet_generation['Photovoltaics [MWh] Calculated resolutions'], errors='coerce')\n",
    "\n",
    "trsnet_generation['electricity_generated'] = (\n",
    "    trsnet_generation['Wind onshore [MWh] Calculated resolutions'] +\n",
    "    trsnet_generation['Photovoltaics [MWh] Calculated resolutions']\n",
    ")\n",
    "\n",
    "trsnet_generation = trsnet_generation[['electricity_generated']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd5aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df_transformed.join(trsnet_generation['electricity_generated'], how='left')\n",
    "\n",
    "new_column_order = [\n",
    "    'Total_Load', 'Residual_Load', 'electricity_generated',  # Electricity-related columns\n",
    "    'Air_Temperature', 'Relative_Humidity', 'Visibility', 'Air_Pressure_at_Station_Height', \n",
    "    'Cloud_Cover', 'Daily_Precipitation_Height', 'Snow_Height_Daily_Value', 'Global_Radiation',  \n",
    "    'wind_u', 'wind_v',  # Weather-related columns\n",
    "    'dayofweek', 'quarter', 'month', 'year', 'dayofyear', 'holiday'  # Temporal features\n",
    "]\n",
    "\n",
    "df_transformed = df_transformed[new_column_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bfd9c5",
   "metadata": {},
   "source": [
    "### Add Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a65e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_map = df_transformed['Residual_Load'].to_dict()\n",
    "df_transformed['Residual_Load_Tomorrow']= (df_transformed.index + pd.Timedelta(unit='days', value=1)).map(target_map)\n",
    "df_transformed.dropna(inplace=True)\n",
    "df_transformed.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236ebecb",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "### Function for Training XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10a79aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "\n",
    "def train_xgboost_model(X, y, enable_categorical=True, n_splits=5, n_estimators=500, learning_rate=0.05, \n",
    "                         max_depth=6, min_child_weight=5, subsample=0.8, colsample_bytree=0.8, gamma=0.1, \n",
    "                         early_stopping_rounds=20, random_state=42):\n",
    "    \"\"\"\n",
    "    Train an XGBoost model with optimized hyperparameters and time-series cross-validation.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Features (DataFrame)\n",
    "    - y: Target variable (Series)\n",
    "    - enable_categorical: Enables categorical support for XGBoost.\n",
    "    - n_splits: Number of splits for TimeSeriesSplit.\n",
    "    - n_estimators: Number of trees in the XGBoost model.\n",
    "    - learning_rate: Step size for XGBoost model.\n",
    "    - max_depth: Maximum depth of each tree.\n",
    "    - min_child_weight: Minimum weight required to split a node.\n",
    "    - subsample: Fraction of data used per boosting round.\n",
    "    - colsample_bytree: Fraction of features used per tree.\n",
    "    - gamma: Minimum loss reduction required to make a further partition.\n",
    "    - early_stopping_rounds: Stops training if validation error doesn't improve.\n",
    "    - random_state: Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - avg_mae: Average Mean Absolute Error across all folds.\n",
    "    - avg_rmse: Average Root Mean Squared Error across all folds.\n",
    "    - avg_r2: Average R² Score across all folds.\n",
    "    - avg_mape: Average Mean Absolute Percentage Error across all folds.\n",
    "    - avg_smape: Average Symmetric MAPE across all folds.\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    xgb_mae, xgb_rmse, xgb_r2, xgb_mape, xgb_smape = [], [], [], [], []\n",
    "\n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # Train XGBoost model with optimized hyperparameters\n",
    "        xgb_model = XGBRegressor(\n",
    "            enable_categorical=enable_categorical, \n",
    "            n_estimators=n_estimators, \n",
    "            learning_rate=learning_rate, \n",
    "            max_depth=max_depth,\n",
    "            min_child_weight=min_child_weight,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            gamma=gamma,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=early_stopping_rounds, verbose=False)\n",
    "        xgb_pred = xgb_model.predict(X_test)\n",
    "\n",
    "        # Evaluate performance\n",
    "        xgb_mae.append(mean_absolute_error(y_test, xgb_pred))\n",
    "        xgb_rmse.append(np.sqrt(mean_squared_error(y_test, xgb_pred)))\n",
    "        xgb_r2.append(r2_score(y_test, xgb_pred))\n",
    "\n",
    "        mape = np.mean(np.abs((y_test - xgb_pred) / y_test)) * 100\n",
    "        xgb_mape.append(mape)\n",
    "\n",
    "        smape = np.mean(2 * np.abs(y_test - xgb_pred) / (np.abs(y_test) + np.abs(xgb_pred))) * 100\n",
    "        xgb_smape.append(smape)\n",
    "\n",
    "    # Calculate average scores\n",
    "    avg_mae = np.mean(xgb_mae)\n",
    "    avg_rmse = np.mean(xgb_rmse)\n",
    "    avg_r2 = np.mean(xgb_r2)\n",
    "    avg_mape = np.mean(xgb_mape)\n",
    "    avg_smape = np.mean(xgb_smape)\n",
    "\n",
    "    # Print results\n",
    "    print(\"--- Optimized XGBoost Cross-Validation Results ---\")\n",
    "    print(f\"Avg MAE: {avg_mae:.2f} (in MWh)\")\n",
    "    print(f\"Avg RMSE: {avg_rmse:.2f} (in MWh)\")\n",
    "    print(f\"Avg R² Score: {avg_r2:.2f}\")\n",
    "    print(f\"Avg MAPE: {avg_mape:.2f} %\")\n",
    "    print(f\"Avg SMAPE: {avg_smape:.2f} %\")\n",
    "\n",
    "    return avg_mae, avg_rmse, avg_r2, avg_mape, avg_smape, y_train, y_test, xgb_pred, xgb_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f164c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "\n",
    "def train_xgboost_model(X, y, enable_categorical=True, n_splits=5, n_estimators=800, learning_rate=0.03, \n",
    "                         max_depth=3, min_child_weight=5, subsample=0.5, colsample_bytree=0.5, gamma=0.5, \n",
    "                         random_state=42):\n",
    "    \"\"\"\n",
    "    Train an XGBoost model with optimized hyperparameters and time-series cross-validation.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Features (DataFrame)\n",
    "    - y: Target variable (Series)\n",
    "    - enable_categorical: Enables categorical support for XGBoost.\n",
    "    - n_splits: Number of splits for TimeSeriesSplit.\n",
    "    - n_estimators: Number of trees in the XGBoost model.\n",
    "    - learning_rate: Step size for XGBoost model.\n",
    "    - max_depth: Maximum depth of each tree.\n",
    "    - min_child_weight: Minimum weight required to split a node.\n",
    "    - subsample: Fraction of data used per boosting round.\n",
    "    - colsample_bytree: Fraction of features used per tree.\n",
    "    - gamma: Minimum loss reduction required to make a further partition.\n",
    "    - early_stopping_rounds: Stops training if validation error doesn't improve.\n",
    "    - random_state: Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - avg_mae: Average Mean Absolute Error across all folds.\n",
    "    - avg_rmse: Average Root Mean Squared Error across all folds.\n",
    "    - avg_r2: Average R² Score across all folds.\n",
    "    - avg_mape: Average Mean Absolute Percentage Error across all folds.\n",
    "   \n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    xgb_mae, xgb_rmse, xgb_r2, xgb_mape = [], [], [], []\n",
    "\n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # Train XGBoost model with optimized hyperparameters\n",
    "        xgb_model = XGBRegressor(\n",
    "            enable_categorical=enable_categorical, \n",
    "            n_estimators=n_estimators, \n",
    "            learning_rate=learning_rate, \n",
    "            max_depth=max_depth,\n",
    "            min_child_weight=min_child_weight,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            gamma=gamma,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        xgb_model.set_params(eval_metric=\"rmse\")  # Before training, set eval_metric\n",
    "            \n",
    "        xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "        xgb_pred = xgb_model.predict(X_test)        \n",
    "\n",
    "\n",
    "\n",
    "        # Evaluate performance\n",
    "    xgb_mae.append(mean_absolute_error(y_test, xgb_pred))\n",
    "    xgb_rmse.append(np.sqrt(mean_squared_error(y_test, xgb_pred)))\n",
    "    xgb_r2.append(r2_score(y_test, xgb_pred))\n",
    "\n",
    "    mape = np.mean(np.abs((y_test - xgb_pred) / y_test)) * 100\n",
    "    xgb_mape.append(mape)\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate average scores\n",
    "    avg_mae = np.mean(xgb_mae)\n",
    "    avg_rmse = np.mean(xgb_rmse)\n",
    "    avg_r2 = np.mean(xgb_r2)\n",
    "    avg_mape = np.mean(xgb_mape)\n",
    "  \n",
    "\n",
    "    # Print results\n",
    "    print(\"--- Optimized XGBoost Cross-Validation Results ---\")\n",
    "    print(f\"Avg MAE: {avg_mae:.2f} (in MWh)\")\n",
    "    print(f\"Avg RMSE: {avg_rmse:.2f} (in MWh)\")\n",
    "    print(f\"Avg R² Score: {avg_r2:.2f}\")\n",
    "    print(f\"Avg MAPE: {avg_mape:.2f} %\")\n",
    " \n",
    "\n",
    "    return avg_mae, avg_rmse, avg_r2, avg_mape, y_train, y_test, xgb_pred, xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93abaedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "print(xgboost.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ac332a",
   "metadata": {},
   "source": [
    "### Function for Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52b65b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Define the plotting function\n",
    "def plot_predictions(y_train, y_test, xgb_pred, start_date, end_date):\n",
    "    # Prepare the dataframes\n",
    "    y_train_df = pd.DataFrame(y_train).rename(columns={y_train.name: 'y_train'})\n",
    "    y_test_df = pd.DataFrame(y_test).rename(columns={y_test.name: 'y_test'})\n",
    "    xgb_pred_df = pd.DataFrame(xgb_pred, index=y_test.index).rename(columns={0: 'xgb_pred'})\n",
    "\n",
    "    # Merge them all into a single DataFrame\n",
    "    combined_df = pd.concat([y_train_df, y_test_df, xgb_pred_df], axis=1)\n",
    "    filtered_df = combined_df.loc[start_date:end_date]\n",
    "\n",
    "    # Plotting the combined DataFrame\n",
    "    plt.figure(figsize=(12, 6)) #Addjusted Size (Difference Original)\n",
    "\n",
    "    plt.plot(filtered_df.index, filtered_df['y_train'], label='y_train', color='blue')\n",
    "    plt.plot(filtered_df.index, filtered_df['y_test'], label='y_test', color='green')\n",
    "    plt.plot(filtered_df.index, filtered_df['xgb_pred'], label='XGBoost Predicted', color='red')\n",
    "\n",
    "    plt.title('True vs Predicted Values')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Residual Load')\n",
    "\n",
    "    # # Format the x-axis to show daily ticks\n",
    "    plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=5))  # Set major ticks at daily intervals\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  # Format date on x-axis\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b43d1",
   "metadata": {},
   "source": [
    "## Train on Available Features All At Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3fe08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "X = df_transformed.drop(columns=['Residual_Load_Tomorrow'])\n",
    "\n",
    "y = df_transformed['Residual_Load_Tomorrow']\n",
    "\n",
    "print(f\"X columns are: {X.columns}\\n\")\n",
    "print(f\"y shape is: {y.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b957b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "\n",
    "xgb = XGBRegressor(enable_categorical=True)\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb, param_grid, cv=5, scoring=\"neg_mean_squared_error\", n_iter=20, n_jobs=-1\n",
    ")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "for train_idx, test_idx in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "random_search.fit(X_train, y_train)\n",
    "print(random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cd92f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "    \"learning_rate\": [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1],\n",
    "    \"max_depth\": [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"min_child_weight\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"subsample\": [0.5, 0.7, 1],\n",
    "    \"colsample_bytree\": [0.5, 0.7, 1],\n",
    "    \"gamma\": [0, 0.1, 0.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8090f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, validate the model and extract relevant returned values\n",
    "avg_mae, avg_rmse, avg_r2, avg_mape, y_train, y_test, xgb_pred, xgb_model = train_xgboost_model(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39114850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting prediction results\n",
    "start_date = '2023-07-01'\n",
    "end_date = y_test.index[-1]\n",
    "plot_predictions(y_train, y_test, xgb_pred, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735290de",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b889bea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.DataFrame(data=xgb_model.feature_importances_, index=xgb_model.feature_names_in_, columns=['importance'])\n",
    "fi.sort_values('importance').plot(kind='barh', title='Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b777a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
