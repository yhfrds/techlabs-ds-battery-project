{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Mean Absolute Percentage Error (MAPE)**, **Mean Absolute Error (MAE)**, and **Root Mean Squared Error (RMSE)** are three common metrics used to measure the accuracy of time series forecasts. Here's a comparison of these metrics:\n",
        "\n",
        "1. **MAPE (Mean Absolute Percentage Error)**:\n",
        "   - **Formula**: $$ \\frac{100\\%}{n} \\sum \\left| \\frac{Actual - Forecast}{Actual} \\right| $$\n",
        "   - It's a relative metric, expressed in percentage terms.\n",
        "   - Can be infinite or undefined if there are actual values that are zero.\n",
        "   - Often preferred when comparing the relative errors across multiple series of different scales.\n",
        "\n",
        "2. **MAE (Mean Absolute Error)**:\n",
        "   - **Formula**: $$ \\frac{1}{n} \\sum |Actual - Forecast| $$\n",
        "   - Measures the average magnitude of errors, regardless of their direction.\n",
        "   - Has the same scale as the data.\n",
        "\n",
        "3. **RMSE (Root Mean Squared Error)**:\n",
        "   - **Formula**: $$ \\sqrt{\\frac{1}{n} \\sum (Actual - Forecast)^2} $$\n",
        "   - Sensitive to outliers, i.e., large errors have a disproportionately large impact on RMSE.\n",
        "   - Useful when large errors are particularly undesirable.\n",
        "\n",
        "**Python Code Example**:\n",
        "Let's calculate these errors for a simple time series dataset:\n",
        "\n"
      ],
      "metadata": {
        "id": "RQ2IM16ymEWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "actual = np.array([100, 200, 300, 400, 0])\n",
        "forecast = np.array([110, 190, 310, 410, 490])\n",
        "\n",
        "# MAPE\n",
        "mape = np.mean(np.abs((actual - forecast) / actual)) * 100\n",
        "print(f\"MAPE: {mape:.2f}%\")\n",
        "\n",
        "# MAE\n",
        "mae = np.mean(np.abs(actual - forecast))\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "\n",
        "# RMSE\n",
        "rmse = np.sqrt(np.mean((actual - forecast) ** 2))\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zKHRkghimgdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39ee26d3-9aa2-4c14-e2c3-b98e2d1a9086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE: inf%\n",
            "MAE: 106.00\n",
            "RMSE: 219.32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-e2a851b2e64b>:7: RuntimeWarning: divide by zero encountered in divide\n",
            "  mape = np.mean(np.abs((actual - forecast) / actual)) * 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of time series:\n",
        "- **MAPE** can be particularly helpful if you're more interested in relative forecast accuracy rather than absolute error. For instance, a 5% error may be considered high for one product's sales forecast but low for another.\n",
        "  \n",
        "- **MAE** gives you a direct average error magnitude, so it's more interpretable in terms of the raw data.\n",
        "  \n",
        "- **RMSE** gives more weight to larger errors, so if bigger mistakes are more costly in your specific application, RMSE might be more appropriate.\n",
        "\n",
        "In choosing among these, consider the specifics of the application, the scale of the data, and how the results will be used or interpreted."
      ],
      "metadata": {
        "id": "0BKQYoxImkLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Relative Forecasts\n",
        "\n",
        "Relative forecast accuracy refers to the measurement of forecast errors in proportion to another benchmark or standard. It provides a comparative perspective, allowing us to assess how well a forecasting method performs in relation to another method or relative to the size of the numbers being forecasted.\n",
        "\n",
        "For example, if you are predicting sales for a product and the actual sales are 100 units while your forecast was 105 units, the absolute error is 5 units. But the relative error might be 5% (5/100). This percentage gives a sense of scale; a 5-unit error means different things if you're discussing 100 units versus 10,000 units.\n",
        "\n",
        "The concept behind relative forecast accuracy can be crucial in various scenarios:\n",
        "\n",
        "1. **Comparing Different Time Series**: If you have multiple products or time series to forecast, each might be of a different scale. Using relative accuracy allows you to compare these different scales more meaningfully.\n",
        "\n",
        "2. **Comparing Models**: When testing multiple forecasting models, relative accuracy can help determine which model performs better proportionally, regardless of the magnitude of the actual values.\n",
        "\n",
        "3. **Communicating with Stakeholders**: In business contexts, stakeholders might be more interested in percentage terms (\"We're off by 5%\") than absolute values (\"We missed the forecast by 50 units\").\n",
        "\n",
        "One common metric used to gauge relative forecast accuracy is the Mean Absolute Percentage Error (MAPE). It measures the average of the absolute percentage errors:\n",
        "\n",
        "$$ MAPE = \\frac{100\\%}{n} \\sum \\left| \\frac{Actual - Forecast}{Actual} \\right| $$\n",
        "\n",
        "However, while relative forecast accuracy metrics like MAPE are valuable, they come with limitations. For instance, MAPE becomes problematic when the actual value is close to zero, as the denominator can cause the error percentage to inflate dramatically or become undefined."
      ],
      "metadata": {
        "id": "GQc2EP1imKVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise\n",
        "\n",
        "* Insert zero values in the `actual` time series. What happens with MAPE, MAE, and RMSE then? How would you avoid this issue?"
      ],
      "metadata": {
        "id": "IrnBGRfioOVi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BPyuP9YyuuyX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}